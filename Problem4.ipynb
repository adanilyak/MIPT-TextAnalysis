{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.is_blocked.mean()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print \"Blocked ratio\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274996\n",
      "274996\n"
     ]
    }
   ],
   "source": [
    "dfBlocked = df[df.is_blocked == 1]\n",
    "dfNotBlockedDownsampled = df[df.is_blocked == 0].sample(n=len(dfBlocked))\n",
    "print len(dfBlocked)\n",
    "print len(dfNotBlockedDownsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.5\n",
      "Count: 549992\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "df = pd.concat([dfBlocked, dfNotBlockedDownsampled])\n",
    "\n",
    "print \"Blocked ratio:\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x138a74cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "min_count = 10\n",
    "tokens = [k for k, v in token_counts.items() if v >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 87978\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) > 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 15)\n",
      "Поездки на таможню, печать в паспорте -> [43313 14722 55422 82168 80250 17381     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [ 8397     0 30544     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [28915 23486     0  3659 34048     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "categories = [{\"category\": i[0],\"subcategory\": i[1]} for i in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = \\\n",
    "train_test_split(title_tokens, desc_tokens, df_non_text, target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed data (may take up to 3 minutes)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4fe850f40f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preprocessed_data.pcl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"token_to_id.pcl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_tuple' is not defined"
     ]
    }
   ],
   "source": [
    "save_prepared_data = True #save\n",
    "read_prepared_data = False #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"готово\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print \"done\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn_1 = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=64)\n",
    "descr_nn_2 = lasagne.layers.LSTMLayer(descr_nn_1, 3, grad_clipping=5)\n",
    "descr_nn_3 = lasagne.layers.FlattenLayer(descr_nn_2)\n",
    "\n",
    "# Titles\n",
    "title_nn_1 = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=64)\n",
    "title_nn_2 = lasagne.layers.LSTMLayer(title_nn_1, 3, grad_clipping=5)\n",
    "title_nn_3 = lasagne.layers.FlattenLayer(title_nn_2)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=64, nonlinearity=lasagne.nonlinearities.leaky_rectify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat = lasagne.layers.concat([descr_nn_3, title_nn_3, cat_nn]) \n",
    "\n",
    "dense = lasagne.layers.DenseLayer(concat,1234)\n",
    "dropout = lasagne.layers.DropoutLayer(dense,p=0.57)\n",
    "dense_output = lasagne.layers.DenseLayer(dropout,1,nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(dense_output,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(dense_output)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction, target_y, delta = 1.0, log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adam(loss, params=weights, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(dense_output,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction, target_y, delta = 1.0, log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alexander/anaconda/envs/python2/lib/python2.7/site-packages/theano/tensor/basic.py:5130: UserWarning: flatten outdim parameter is deprecated, use ndim instead.\n",
      "  \"flatten outdim parameter is deprecated, use ndim instead.\")\n"
     ]
    }
   ],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 9389.18241664\n",
      "\tacc: 0.843199029126\n",
      "\tauc: 0.870537472378\n",
      "\tap@k: 0.103049417731\n",
      "Val:\n",
      "\tloss: 2390.81883188\n",
      "\tacc: 0.868642335766\n",
      "\tauc: 0.888066170437\n",
      "\tap@k: 0.98014517928\n",
      "Train:\n",
      "\tloss: 2642.22310925\n",
      "\tacc: 0.893330097087\n",
      "\tauc: 0.932059259411\n",
      "\tap@k: 0.195473273433\n",
      "Val:\n",
      "\tloss: 295.57858799\n",
      "\tacc: 0.896445255474\n",
      "\tauc: 0.938914801406\n",
      "\tap@k: 0.984491848239\n",
      "Train:\n",
      "\tloss: 376.341839231\n",
      "\tacc: 0.911669902913\n",
      "\tauc: 0.95244151029\n",
      "\tap@k: 0.478410375711\n",
      "Val:\n",
      "\tloss: 7.43218329125\n",
      "\tacc: 0.905671532847\n",
      "\tauc: 0.94555357704\n",
      "\tap@k: 0.931138370863\n",
      "Train:\n",
      "\tloss: 1.08765906385\n",
      "\tacc: 0.933233009709\n",
      "\tauc: 0.976508564172\n",
      "\tap@k: 0.993527854375\n",
      "Val:\n",
      "\tloss: 1.78903995582\n",
      "\tacc: 0.90796350365\n",
      "\tauc: 0.968507487474\n",
      "\tap@k: 0.977606554766\n",
      "Train:\n",
      "\tloss: 0.189770164604\n",
      "\tacc: 0.943274271845\n",
      "\tauc: 0.980484550214\n",
      "\tap@k: 0.999891880756\n",
      "Val:\n",
      "\tloss: 1.25289513842\n",
      "\tacc: 0.90002189781\n",
      "\tauc: 0.963790274052\n",
      "\tap@k: 0.986272706385\n",
      "Train:\n",
      "\tloss: 0.142748063305\n",
      "\tacc: 0.948946601942\n",
      "\tauc: 0.981393126247\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.942504689514\n",
      "\tacc: 0.907773722628\n",
      "\tauc: 0.969233052247\n",
      "\tap@k: 0.989508167551\n",
      "Train:\n",
      "\tloss: 0.15751514015\n",
      "\tacc: 0.950461165049\n",
      "\tauc: 0.978624137817\n",
      "\tap@k: 0.999118471837\n",
      "Val:\n",
      "\tloss: 0.648621175484\n",
      "\tacc: 0.919102189781\n",
      "\tauc: 0.970313696447\n",
      "\tap@k: 0.992907485213\n",
      "Train:\n",
      "\tloss: 0.111334006406\n",
      "\tacc: 0.95659223301\n",
      "\tauc: 0.980231582646\n",
      "\tap@k: 0.999965569063\n",
      "Val:\n",
      "\tloss: 0.625252129652\n",
      "\tacc: 0.924817518248\n",
      "\tauc: 0.971561391566\n",
      "\tap@k: 0.994396261513\n",
      "Train:\n",
      "\tloss: 0.0960139771298\n",
      "\tacc: 0.961298543689\n",
      "\tauc: 0.982496319021\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.544498639822\n",
      "\tacc: 0.907189781022\n",
      "\tauc: 0.962132420377\n",
      "\tap@k: 0.994760486371\n",
      "Train:\n",
      "\tloss: 0.0961393082028\n",
      "\tacc: 0.963405339806\n",
      "\tauc: 0.983583645606\n",
      "\tap@k: 0.999904239724\n",
      "Val:\n",
      "\tloss: 0.556282031576\n",
      "\tacc: 0.909452554745\n",
      "\tauc: 0.959217555027\n",
      "\tap@k: 0.995626218757\n",
      "Train:\n",
      "\tloss: 0.0766874004978\n",
      "\tacc: 0.968594660194\n",
      "\tauc: 0.985500980237\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.625848109647\n",
      "\tacc: 0.907474452555\n",
      "\tauc: 0.955802971658\n",
      "\tap@k: 0.992579578316\n",
      "Train:\n",
      "\tloss: 0.0686418562527\n",
      "\tacc: 0.971808252427\n",
      "\tauc: 0.986923830393\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.495635881606\n",
      "\tacc: 0.91195620438\n",
      "\tauc: 0.96756678478\n",
      "\tap@k: 0.994912067878\n",
      "Train:\n",
      "\tloss: 104246.691865\n",
      "\tacc: 0.910747572816\n",
      "\tauc: 0.90754123518\n",
      "\tap@k: 0.224971880681\n",
      "Val:\n",
      "\tloss: 123451.969023\n",
      "\tacc: 0.864854014599\n",
      "\tauc: 0.86288186179\n",
      "\tap@k: 0.998133427663\n",
      "Train:\n",
      "\tloss: 47185.1108407\n",
      "\tacc: 0.902186893204\n",
      "\tauc: 0.939183003081\n",
      "\tap@k: 0.267924837464\n",
      "Val:\n",
      "\tloss: 45305.0888758\n",
      "\tacc: 0.870408759124\n",
      "\tauc: 0.892522249813\n",
      "\tap@k: 0.99809414043\n",
      "Train:\n",
      "\tloss: 15598.4777354\n",
      "\tacc: 0.912344660194\n",
      "\tauc: 0.957269228852\n",
      "\tap@k: 0.491619576844\n",
      "Val:\n",
      "\tloss: 10742.2950493\n",
      "\tacc: 0.892919708029\n",
      "\tauc: 0.928229366467\n",
      "\tap@k: 0.998621630476\n",
      "Train:\n",
      "\tloss: 3931.03481224\n",
      "\tacc: 0.936944174757\n",
      "\tauc: 0.969608351483\n",
      "\tap@k: 0.765192533805\n",
      "Val:\n",
      "\tloss: 459.83712274\n",
      "\tacc: 0.921145985401\n",
      "\tauc: 0.967038612304\n",
      "\tap@k: 0.999317077899\n",
      "Train:\n",
      "\tloss: 1196.53138667\n",
      "\tacc: 0.936283980583\n",
      "\tauc: 0.972844927456\n",
      "\tap@k: 0.909094467991\n",
      "Val:\n",
      "\tloss: 891.841565905\n",
      "\tacc: 0.898248175182\n",
      "\tauc: 0.932156834532\n",
      "\tap@k: 0.905054443636\n",
      "Train:\n",
      "\tloss: 155.150428833\n",
      "\tacc: 0.929672330097\n",
      "\tauc: 0.972597188573\n",
      "\tap@k: 0.985136927932\n",
      "Val:\n",
      "\tloss: 58.5000224204\n",
      "\tacc: 0.91200729927\n",
      "\tauc: 0.956592478019\n",
      "\tap@k: 0.999245182674\n",
      "Train:\n",
      "\tloss: 6.8834842157\n",
      "\tacc: 0.925808252427\n",
      "\tauc: 0.967734504561\n",
      "\tap@k: 0.999610941774\n",
      "Val:\n",
      "\tloss: 29.5844690365\n",
      "\tacc: 0.921189781022\n",
      "\tauc: 0.965675180647\n",
      "\tap@k: 0.999319107402\n",
      "Train:\n",
      "\tloss: 3.61004303299\n",
      "\tacc: 0.930283980583\n",
      "\tauc: 0.969349347043\n",
      "\tap@k: 0.999767681763\n",
      "Val:\n",
      "\tloss: 29.3619310578\n",
      "\tacc: 0.932204379562\n",
      "\tauc: 0.970764753337\n",
      "\tap@k: 0.999218671216\n",
      "Train:\n",
      "\tloss: 1.38797628408\n",
      "\tacc: 0.919851941748\n",
      "\tauc: 0.96385134214\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 14.4047979301\n",
      "\tacc: 0.900350364964\n",
      "\tauc: 0.952137708628\n",
      "\tap@k: 0.998996453609\n",
      "Train:\n",
      "\tloss: 0.224270205141\n",
      "\tacc: 0.924347087379\n",
      "\tauc: 0.964477444497\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 13.5760344823\n",
      "\tacc: 0.928802919708\n",
      "\tauc: 0.972536328451\n",
      "\tap@k: 0.99891883736\n",
      "Train:\n",
      "\tloss: 0.176596778854\n",
      "\tacc: 0.936550970874\n",
      "\tauc: 0.971856518104\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 13.5695681778\n",
      "\tacc: 0.91697080292\n",
      "\tauc: 0.961193164386\n",
      "\tap@k: 0.99900833241\n",
      "Train:\n",
      "\tloss: 0.161995974003\n",
      "\tacc: 0.932759708738\n",
      "\tauc: 0.969667371292\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 13.9809189803\n",
      "\tacc: 0.94195620438\n",
      "\tauc: 0.977210542396\n",
      "\tap@k: 0.999208420572\n",
      "Train:\n",
      "\tloss: 0.156117141735\n",
      "\tacc: 0.944439320388\n",
      "\tauc: 0.977558374537\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 13.6655167349\n",
      "\tacc: 0.913729927007\n",
      "\tauc: 0.957909534189\n",
      "\tap@k: 0.999680896059\n",
      "Train:\n",
      "\tloss: 0.209134446981\n",
      "\tacc: 0.935356796117\n",
      "\tauc: 0.969864670044\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 9.12943748561\n",
      "\tacc: 0.904240875912\n",
      "\tauc: 0.971936298882\n",
      "\tap@k: 0.998959958893\n",
      "Train:\n",
      "\tloss: 0.252629453388\n",
      "\tacc: 0.920264563107\n",
      "\tauc: 0.95866155893\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 9.55369881088\n",
      "\tacc: 0.927912408759\n",
      "\tauc: 0.968263288937\n",
      "\tap@k: 0.998968261924\n",
      "Train:\n",
      "\tloss: 6314.61128471\n",
      "\tacc: 0.903461165049\n",
      "\tauc: 0.941997319648\n",
      "\tap@k: 0.697054510998\n",
      "Val:\n",
      "\tloss: 36083.5097998\n",
      "\tacc: 0.86802919708\n",
      "\tauc: 0.883511634356\n",
      "\tap@k: 0.99818531633\n",
      "Train:\n",
      "\tloss: 19495.2293521\n",
      "\tacc: 0.895165048544\n",
      "\tauc: 0.94691552191\n",
      "\tap@k: 0.480224304887\n",
      "Val:\n",
      "\tloss: 133.285870378\n",
      "\tacc: 0.883510948905\n",
      "\tauc: 0.959927097524\n",
      "\tap@k: 0.999102664954\n",
      "Train:\n",
      "\tloss: 60.1534555928\n",
      "\tacc: 0.890876213592\n",
      "\tauc: 0.961215580206\n",
      "\tap@k: 0.99364060775\n",
      "Val:\n",
      "\tloss: 39.1208184626\n",
      "\tacc: 0.890248175182\n",
      "\tauc: 0.933406664951\n",
      "\tap@k: 0.995661106694\n",
      "Train:\n",
      "\tloss: 2.01991837292\n",
      "\tacc: 0.878029126214\n",
      "\tauc: 0.951529243843\n",
      "\tap@k: 0.999971155572\n",
      "Val:\n",
      "\tloss: 17.3808017931\n",
      "\tacc: 0.878277372263\n",
      "\tauc: 0.955629363745\n",
      "\tap@k: 0.999588394746\n",
      "Train:\n",
      "\tloss: 0.55273592303\n",
      "\tacc: 0.837140776699\n",
      "\tauc: 0.930127756322\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 11.6187240268\n",
      "\tacc: 0.870364963504\n",
      "\tauc: 0.940813066154\n",
      "\tap@k: 0.998973321611\n",
      "Train:\n",
      "\tloss: 0.388057161438\n",
      "\tacc: 0.833514563107\n",
      "\tauc: 0.926056257354\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 13.1444312347\n",
      "\tacc: 0.880255474453\n",
      "\tauc: 0.94889579619\n",
      "\tap@k: 0.998952575864\n",
      "Train:\n",
      "\tloss: 0.337142947589\n",
      "\tacc: 0.861449029126\n",
      "\tauc: 0.939164330312\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 15.1539687359\n",
      "\tacc: 0.889153284672\n",
      "\tauc: 0.950367139919\n",
      "\tap@k: 0.998905337786\n",
      "Train:\n",
      "\tloss: 0.367243099956\n",
      "\tacc: 0.866007281553\n",
      "\tauc: 0.938191079782\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 6.1873411919\n",
      "\tacc: 0.876248175182\n",
      "\tauc: 0.937170554756\n",
      "\tap@k: 0.999513643874\n",
      "Train:\n",
      "\tloss: 0.373253140052\n",
      "\tacc: 0.834599514563\n",
      "\tauc: 0.909339462018\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 6.63206307912\n",
      "\tacc: 0.880248175182\n",
      "\tauc: 0.937695809009\n",
      "\tap@k: 0.999750589655\n",
      "Train:\n",
      "\tloss: 0.32898276865\n",
      "\tacc: 0.852276699029\n",
      "\tauc: 0.918780343257\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 7.25620318377\n",
      "\tacc: 0.94000729927\n",
      "\tauc: 0.977684750305\n",
      "\tap@k: 0.999774099195\n",
      "Train:\n",
      "\tloss: 0.37702056617\n",
      "\tacc: 0.835580097087\n",
      "\tauc: 0.91027030302\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 7.08776508102\n",
      "\tacc: 0.87904379562\n",
      "\tauc: 0.934816320438\n",
      "\tap@k: 0.999829244228\n",
      "Train:\n",
      "\tloss: 0.3328287139\n",
      "\tacc: 0.845194174757\n",
      "\tauc: 0.918696778292\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 8.56763305337\n",
      "\tacc: 0.885934306569\n",
      "\tauc: 0.940871935276\n",
      "\tap@k: 0.999888989903\n",
      "Train:\n",
      "\tloss: 934.509105763\n",
      "\tacc: 0.856723300971\n",
      "\tauc: 0.929529176548\n",
      "\tap@k: 0.810736712414\n",
      "Val:\n",
      "\tloss: 144.446226178\n",
      "\tacc: 0.834379562044\n",
      "\tauc: 0.894639588916\n",
      "\tap@k: 0.993169799979\n",
      "Train:\n",
      "\tloss: 1.44695134221\n",
      "\tacc: 0.807747572816\n",
      "\tauc: 0.917894529015\n",
      "\tap@k: 0.999413223628\n",
      "Val:\n",
      "\tloss: 10.8791998338\n",
      "\tacc: 0.79495620438\n",
      "\tauc: 0.934139354648\n",
      "\tap@k: 0.998259994657\n",
      "Train:\n",
      "\tloss: 0.54651652909\n",
      "\tacc: 0.859276699029\n",
      "\tauc: 0.924811080397\n",
      "\tap@k: 0.999928416449\n",
      "Val:\n",
      "\tloss: 8.45062109518\n",
      "\tacc: 0.81899270073\n",
      "\tauc: 0.932717147839\n",
      "\tap@k: 0.999242976313\n",
      "Train:\n",
      "\tloss: 0.290706962174\n",
      "\tacc: 0.881213592233\n",
      "\tauc: 0.931953109619\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 7.2623106296\n",
      "\tacc: 0.839335766423\n",
      "\tauc: 0.933641796828\n",
      "\tap@k: 0.999097762173\n",
      "Train:\n",
      "\tloss: 0.246492085804\n",
      "\tacc: 0.899337378641\n",
      "\tauc: 0.940156054257\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 6.7595803468\n",
      "\tacc: 0.851868613139\n",
      "\tauc: 0.933641667362\n",
      "\tap@k: 0.999114387928\n",
      "Train:\n",
      "\tloss: 0.226338130697\n",
      "\tacc: 0.909004854369\n",
      "\tauc: 0.943672907967\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 8.37451545825\n",
      "\tacc: 0.843255474453\n",
      "\tauc: 0.933574179596\n",
      "\tap@k: 0.999313577011\n",
      "Train:\n",
      "\tloss: 0.625267612904\n",
      "\tacc: 0.914691747573\n",
      "\tauc: 0.94546115258\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 8.46759625314\n",
      "\tacc: 0.836540145985\n",
      "\tauc: 0.913535535249\n",
      "\tap@k: 0.999052497002\n",
      "Train:\n",
      "\tloss: 0.354550275094\n",
      "\tacc: 0.907254854369\n",
      "\tauc: 0.936693674533\n",
      "\tap@k: 0.999987527853\n",
      "Val:\n",
      "\tloss: 9.35639345112\n",
      "\tacc: 0.822131386861\n",
      "\tauc: 0.906887599455\n",
      "\tap@k: 0.999561210242\n",
      "Train:\n",
      "\tloss: 0.508587738306\n",
      "\tacc: 0.91290776699\n",
      "\tauc: 0.941318081689\n",
      "\tap@k: 0.999859686432\n",
      "Val:\n",
      "\tloss: 9.1153232001\n",
      "\tacc: 0.851372262774\n",
      "\tauc: 0.916400415486\n",
      "\tap@k: 0.999229107039\n",
      "Train:\n",
      "\tloss: 0.505246522362\n",
      "\tacc: 0.914533980583\n",
      "\tauc: 0.940638837138\n",
      "\tap@k: 0.999201663198\n",
      "Val:\n",
      "\tloss: 9.98293772781\n",
      "\tacc: 0.855569343066\n",
      "\tauc: 0.915231688392\n",
      "\tap@k: 0.999410949221\n",
      "Train:\n",
      "\tloss: 0.225511034509\n",
      "\tacc: 0.915398058252\n",
      "\tauc: 0.935554077055\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 7.30167466544\n",
      "\tacc: 0.851291970803\n",
      "\tauc: 0.9181334944\n",
      "\tap@k: 0.998926143912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 500\n",
    "minibatches_per_epoch = 1000\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr.as_matrix(),target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr.as_matrix(),target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \n"
     ]
    }
   ],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 7.30084343382\n",
      "\tacc: 0.851350364964\n",
      "\tauc: 0.918191006814\n",
      "\tap@k: 0.998924883017\n",
      "\n",
      "AUC:\n",
      "\tНеплохо, но ты можешь лучше! (not ok)\n",
      "\n",
      "Accuracy:\n",
      "Надо бы подтянуть. (not ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tЗасабмить на kaggle! (great) \n",
      "\t Нет, ну честно - выкачай avito_test.tsv, засабмить и скажи, что вышло.\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr.as_matrix(),target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
